{
  "phase": 1,
  "current_step": "checkpoint_E",
  "status": "in_progress",
  "steps": {
    "prompt_1": {
      "status": "completed",
      "tests_passed": true,
      "notes": "12/12 tests pass. Key decisions: (1) name column nullable to allow NULL rows per fixture. (2) Settings adds cache_llm_responses + log_level. (3) Timeout via threading.Thread + conn.interrupt() (not SIGALRM — macOS compat). (4) bird_loader tolerates alternative SQL field names. (5) 6 tests for bird_loader, 6 for database."
    },
    "prompt_2": {
      "status": "completed",
      "tests_passed": true,
      "notes": "31 tests pass. Key decisions: (1) datasketch.MinHash.hashvalues returns numpy.uint64 — convert via [int(v) for v in mh.hashvalues]. (2) SQLite type affinity: 5-rule algorithm. (3) avg_length only for TEXT; min/max/avg_value only for INTEGER/REAL/NUMERIC. (4) Empty tables: total_count=0, null_rate=0.0 guard. (5) JSON cache via dataclasses.asdict() + json.dump(default=str). (6) minhash_bands: character 3-grams; strings <=2 chars also update mh directly."
    },
    "prompt_3":      {"status": "completed",  "tests_passed": true,  "notes": "10/10 tests pass. Key decisions: (1) Batching via dict keyed by table_name, _BATCH_SIZE=6; (2) Retry decorator wraps inner _call() closure so tenacity reraises correctly; (3) cache_control ephemeral on system prompt block works with anthropic-0.83.0 SDK (passed as list of dicts); (4) Missing columns filled with deterministic default: 'The {col} field in the {table} table.'; (5) Length enforcement via simple slice [:200] / [:1000]; (6) JSON persistence via dataclasses.asdict() matching profiler pattern; (7) anthropic package was not yet installed — added to project dependencies."},
    "prompt_4":      {"status": "completed", "tests_passed": true, "notes": "11/11 tests pass (10 required + 1 bonus). Key decisions: (1) needs_quoting() uses re.compile('^[A-Za-z0-9_]+$') regex — returns True for any name containing spaces, parens, hyphens, slashes, etc. (2) DDL long_summary truncated to 120 chars with '...'; sample row values truncated to 50 chars. (3) NULL sample values: per-column 'NULL' string if sample_values is empty list. (4) Last column in DDL block: trailing comma stripped cleanly — handles both ', --' and bare ',' endings. (5) Markdown sample values: first 3, each truncated to 30 chars with '...'. (6) format_and_save_schemas() creates output_dir if needed, writes {db_id}_ddl.sql and {db_id}_markdown.md. (7) Offline script: --split / --step / --db / --force flags; tqdm progress bar; per-DB error catching; summary at end; indices step deferred to Prompt 6."},
    "checkpoint_A":  {"status": "completed", "requires_user": true, "notes": "All 64 unit tests pass. Improvements applied: (1) summarizer retry logic — on LLMError for batch>1, retries each column individually before falling back to default; (2) warning log emitted per table listing columns that got default fallback; (3) Gemini _parse_response guards for None candidates/content; (4) asyncio.run() fix in offline script. Gemini context caching implemented and tested. Ready for Prompt 5."},
    "prompt_5":      {"status": "completed", "tests_passed": true,  "notes": "10/10 tests pass. Key decisions: (1) LSH threshold lowered to 0.3 (from spec's 0.5) — true 3-gram Jaccard for 'Untied States'/'United States' is ~0.47, below 0.5 so the candidate would be missed; threshold=0.3 retrieves it and post-retrieval Jaccard re-ranking keeps precision high. (2) _minhashes dict maintained alongside MinHashLSH (LSH does not store MinHash objects) for Jaccard re-ranking. (3) _key_meta dict stores (table, column, value) per key for result assembly. (4) Numeric values stored as str(raw_val).strip(). (5) Duplicate keys caught via try/except ValueError on lsh.insert(). (6) sort key = (exact_match, similarity_score) descending — exact matches always float to top. (7) Serialisation via pickle of whole object. (8) Fuzzy test similarity assertion is > 0.4 (not > 0.5) reflecting true 3-gram Jaccard for single-transposition typos."},
    "prompt_6":      {"status": "completed",  "tests_passed": true, "notes": "24/24 tests pass. Key decisions: (1) FAISSIndex uses IndexFlatIP with L2-normalized embeddings (cosine similarity) for DBs <=1000 fields; IndexIVFFlat (nlist=min(32,n//10)) for larger DBs. (2) Similarity scores clipped to [0,1] with np.clip. (3) ExampleStore skeleton masking: collect ALL spans from original text (regex for [NUM], [STR], and spaCy NER for [ENTITY]), resolve overlaps greedily, apply in reverse order — this prevents double-masking (e.g. [NUM] being replaced by [ENTITY]). (4) ExampleStore.query fetches top_k*5 from FAISS then filters by db_id exclusion. (5) OfflineArtifacts pipeline: summary is explicitly saved to disk after summarize_database() call in case mock doesn't write it. (6) Example store is shared across all DBs (not per-DB), saved to indices/example_store.faiss + indices/example_store_metadata.json. (7) train_data empty → _build_example_store logs warning and returns empty store (no error). (8) Skeleton masking examples: 'Find students in Alameda County with GPA above 3.5' → 'Find students in [ENTITY] with [ENTITY] above [NUM]'."},
    "checkpoint_B":  {"status": "completed",  "requires_user": true, "notes": "11/11 BIRD dev databases processed (0 failures). Runtime: ~101 min. Fix applied: LSH caps distinct values per column at 50,000 — this prevented card_games from running 48+ min (now 10 min). Total LSH footprint: 10.86 GB. Issues: (1) LSH indexes non-text cols (addresses, lat/lng, IDs) — bloating index; fix pending user approval; (2) Summarizer batch failures on long-text cols — handled gracefully; (3) SSL event-loop-closed noise — benign. FAISS quality excellent. ExampleStore retrieves structurally similar examples correctly. See checkpoint_B_review/README.md."},
    "prompt_7":      {"status": "completed",  "tests_passed": true,  "notes": "17/17 tests pass. Key decisions: (1) CacheManager._make_key uses SHA256(model + json.dumps(messages, sort_keys=True)) — deterministic regardless of dict key ordering. (2) Cache files named {key[:16]}.json; full key stored inside entry for collision detection. (3) TTL checked at read time only — entries never evicted from disk proactively. (4) enabled=False short-circuits both get() and set() with no disk I/O. (5) cached() decorator extracts messages from first positional arg or 'messages' kwarg. (6) context_grounder uses TYPE_CHECKING guard for CellMatch/ExampleEntry/LSHIndex/ExampleStore imports to avoid circular import risks. (7) ground_context normalises empty/None evidence to the string 'None' before passing to LLM. (8) Deduplication by (table, column) keeps highest similarity_score match when two literals map to same column. (9) ExampleStore.query receives raw question — it performs skeleton masking internally. (10) temperature=0.0 used for deterministic extraction (spec doesn't specify, but 0 is appropriate for extraction tasks)."},
    "prompt_8":      {"status": "completed", "tests_passed": true, "notes": "12/12 tests pass. Key decisions: (1) Two-iteration LLM approach: S1=precise (only definitely needed), S2=recall (wider net). (2) FAISS pre-filter runs first (top_k=settings.faiss_top_k=30), then schema_hints from grounding context augment the candidate list. (3) PKs auto-added by parsing '-- Table:' + 'PRIMARY KEY' in DDL lines; FKs auto-added by parsing '-- Foreign keys:' comment lines. (4) Hallucination filter: each LLM-returned (table, column) is validated against available_field_set before inclusion. (5) S1 ⊆ S2 invariant guaranteed by: s2_extended = s2_extended | s1_extended at end of function. (6) DDL rendering: parse CREATE TABLE blocks per table using _parse_ddl_tables(), rebuild with only selected columns including corrected trailing comma. (7) Markdown rendering: parse ## Table: sections, filter rows by selected column names. (8) Prompt caching: system_block_2 (candidate field list) uses CacheableText(cache=True); system_block_1 (instruction) uses cache=False. (9) Exactly 2 API calls guaranteed even when no remaining candidates (second call made with empty candidate message). (10) All fields in s1_fields and s2_fields are sorted tuples for stable output."},
    "checkpoint_C":  {"status": "completed",  "requires_user": true, "notes": "Tested Ops 5+6 on first 5 BIRD dev questions (all california_schools). 4/5 completed OK. Fixes applied: (1) Gemini 'candidate.content.parts is None' → 'or []' guard; (2) context_grounder: multi-word literals now also query individual words ≥3 chars — fixed critical miss of frpm.County Name for Q1 ('Alameda County' → also queries 'Alameda' which exact-matches County Name). Results: Q1: County Name in S1 ✅; Q2: Educational Option Type in S1 ✅; Q3: Charter School, District Name, Zip in S1 ✅; Q4: FRPM Count, MailStreet in S1 ✅; Q5: Charter Funding Type, Charter School (Y/N), OpenDate, Phone all in S1 ✅. No hallucinations detected. 136/136 tests pass."},
    "prompt_9":      {"status": "completed",  "tests_passed": true, "notes": "10/10 tests pass. Key decisions: (1) clean_sql() strips ```sql ... ``` and ``` ... ``` fences via re.search with DOTALL, then strips trailing semicolons, then collapses whitespace with re.sub('\\s+', ' '). (2) validate_sql_syntax() checks for both SELECT and FROM case-insensitively — returns False for empty string. (3) build_base_prompt() formats question+evidence+cell_matches consistently; cell values shown as table.column = 'value'. (4) ReasoningGenerator uses asyncio.gather() for all 4 concurrent calls; diversity achieved via prompt variation (A1/A3=minimal, A2/A4=step-by-step). (5) No tool-use for reasoning generator — SQL comes in response.text, parsed by clean_sql(). (6) ThinkingConfig(enabled=True, budget_tokens=N) passed to generate(); adaptive budget: 4000 for <=2 tables, 6000 for <=4 tables, 8000 for 5+. (7) response.thinking → reasoning_trace field in SQLCandidate. (8) If response.text is None or clean_sql returns empty → error_flag=True, sql=''. (9) generator_id format: 'reasoning_A1' through 'reasoning_A4'; schema_format always 'ddl'. (10) System prompt includes full DDL schema via CacheableText(cache=True). (11) Step-by-step suffix replaces the final 'Write a SQL query...' line rather than appending after it."},
    "prompt_10":     {"status": "completed",  "tests_passed": true, "notes": "24/24 tests pass (10+10+4). Key decisions: (1) StandardAndComplexGenerator runs all 4 calls (B1_s1, B1_s2, B2_s1, B2_s2) concurrently via asyncio.gather(). (2) B1 uses model_fast, B2 uses model_powerful; neither uses extended thinking (thinking=None). (3) Both B1 and B2 use Markdown schemas (schema_format='markdown'); reasoning generator uses DDL. (4) B2 system prompt explicitly mentions CTEs and window functions to steer the model toward advanced patterns. (5) ICLGenerator formats examples as '## Example N' blocks; applies prompt caching to the examples block (CacheableText(cache=True)). (6) Token guard: len(formatted_examples) // 4 > 6000 → trim to first 6 examples. Used range(400) in test (3510 char SQL) to reliably exceed 6000 token estimate with 8 examples. (7) ICL instruction block has cache=False (schema content per-question), examples block has cache=True (shared across C1/C2/C3). (8) C1='Write the SQL query'; C2='First, identify which tables and joins...'; C3='What is the general SQL pattern...'. (9) Empty few_shot_examples → empty string examples block, still generates 3 candidates. (10) All 3 generators running concurrently produce 4+4+3=11 unique candidates with both S1 and S2 schemas represented."},
    "checkpoint_D":  {"status": "completed",  "requires_user": true, "notes": "Second run (2026-02-23) after targeted fixes. 33-question stratified sample (3 per each of 11 BIRD dev DBs: 1 simple, 1 moderate, 1 challenging, random.seed(42)), runtime ~31 min. Oracle upper bound: 25/33=75.8% (+12.2pp vs run 1: Simple 81.8%, Moderate 63.6%, Challenging 81.8%). Gen success: A=100%/99%exec, B1=100%/95%exec, B2=100%/93%exec, C=95%/94%exec. Duplicate rate: 44% (198 unique/359 total). 6 fixes applied: (1) SQL Writing Rules in base_generator; (2) alt templates + MAX_TOKENS guard + temperature in standard_generator; (3) MAX_TOKENS guard in icl_generator; (4) keyword-extraction fallback on LLMError in context_grounder; (5) S1 try/except + FAISS auto-promotion + caps + S2 skip logic in schema_linker; (6) FK annotations in schema_formatter. Fixed: P0-1, P0-2, P1-3, P1-5; Partially fixed: P1-4, P2-6; Not fixed: P2-7. 8 remaining misses with 5 new issues: New-P1) join-key miss (Player.player_api_id); New-P2) implicit DISTINCT not triggered; New-P3) over-aggregation on 'list and group'; New-P4) binary format 'YES'/'NO' vs natural language; New-P5) GT returns extra columns not implied by question. See checkpoint_D_review/inspection_report.md."},
    "prompt_11":     {"status": "completed", "tests_passed": true,  "notes": "12/12 tests pass. Key decisions: (1) FixedCandidate is a plain dataclass (not Pydantic) consistent with other dataclasses in the project. (2) Error categorization via case-insensitive regex on error_message: 'syntax' → syntax_error, 'no such column/table' → schema_error, 'datatype mismatch' → type_error, is_empty=True → empty_result. (3) Fix prompt uses S2 DDL (maximum context) per spec. (4) Fix calls use model_fast (settings.model_fast — defaults to claude-haiku-4-5-20251001) with temperature=0.0 for determinism. (5) Each candidate's fix loop runs independently as a coroutine; asyncio.gather() in fix_candidates() ensures all candidates are fixed concurrently. (6) Confidence: raw = +1.0 success, +0.5 plausibility (agg→1 row, non-agg→1-100 rows), -0.5 per fix iteration; 0.0 for still-failing. (7) Normalization: only non-zero candidates are normalized; zero-score candidates stay at 0.0; all-tied → all 1.0. (8) clean_sql() from base_generator reused to strip markdown fences from LLM fix responses."},
    "prompt_12":     {"status": "completed", "tests_passed": true,  "notes": "12/12 tests pass. Key decisions: (1) SelectionResult is a plain dataclass with final_sql, selection_method (fast_path/tournament/fallback), tournament_wins dict, confidence, cluster_count, candidates_evaluated. (2) Clustering: rows sorted + normalized (float→f'{v:.6f}', None→'NULL') then str(sorted_rows) as cluster key — ensures order-independent equivalence. (3) Fast path: unanimous 1 cluster → shortest SQL, 0 API calls. (4) Fallback: <2 executable candidates (confidence>0) → return highest confidence candidate, no LLM calls. (5) Representative selection: 1 per cluster (highest confidence, tiebreak: shortest SQL). (6) Sort order for tournament: (is_all_empty, -cluster_size, generator_rank) — empty clusters always last. (7) Generator ranking: reasoning=0, B2/complex=1, icl=2, standard/B1=3 (lower=better). (8) Tournament: C(m,2) pairs run concurrently via asyncio.gather(); higher-ranked rep presented as 'A'. (9) Tool-use via ToolParam with 'select_winner' tool, tool_choice_name='select_winner' forces structured output. (10) Final selection: argmax wins, tiebreakers: cluster size, confidence, generator rank. (11) Fallback on <2 executable — only 1 LLM call test needed confirmation fallback works for 0 executable too. (12) asyncio.get_event_loop().run_in_executor used to run synchronous execute_sql without blocking the event loop."},
    "checkpoint_E":  {"status": "completed",  "requires_user": true, "notes": "RE-RUN v5 (2026-02-24): 33 stratified questions, 2968s (~89.9s/q, 3.1x faster than v4). Accuracy: 17/33 (51.5%) — simple=63.6%, moderate=45.5%, challenging=45.5%. Oracle pre-fix=21/33 (63.6%), oracle post-fix=21/33 (63.6%), selector precision when oracle achievable=17/21 (81.0%). Fixer: 23 candidates needed fix, 7/23 fixed (30.4%), net +5 oracle candidates. Schema recall: table S1=94.95%, S2=94.95%; col S1=93.33%, S2=95.66%; S1 complete=25/33, S2 complete=28/33. Top DBs: debit_card/superhero/toxicology=100%; worst: financial=0% (Czech semantics persists). Code changes tested: (1) generation timeouts eliminated (0 vs 4 in v4) via timeout increase 120→300s; (2) gemini_client multi-step MAX_TOKENS escalation 8192→16384→32768; (3) ICL generator cache=True + MAX_TOKENS retry loop; (4) adaptive_selector sanitize_prompt_text + _format_execution_result; (5) schema_linker_faiss_top_k=50. Remaining issues: (P0-1) 10/16 failures are pure generation errors with complete schemas; (P0-2) tournament selector 50% precision (4/8 oracle misses); (P0-3) financial DB 0% (Czech encoding); (P0-4) schema linker S2 misses budget table in student_club. See checkpoint_E_review/inspection_report_v5.md."},
    "prompt_13":     {"status": "pending",   "tests_passed": null, "notes": "Pipeline: online_pipeline.py + test_online_pipeline.py + run_evaluation.py"},
    "prompt_14":     {"status": "pending",   "tests_passed": null, "notes": "Evaluation: evaluator.py + metrics.py + analyze_results.py + test_evaluator.py"},
    "prompt_15":     {"status": "pending",   "tests_passed": null, "notes": "E2E tests: test_bird_mini.py + test_bird_full.py + run 66-question smoke test (6 per each of 11 databases: 2 simple, 2 moderate, 2 challenging); seeded random selection random.seed(42)"},
    "checkpoint_final": {"status": "pending", "requires_user": true, "notes": "Full BIRD dev evaluation (1534 questions); target EX >= 80%"}
  },
  "notes": {
    "resume_instruction": "Say 'continue implementation' at start of new session. Read this file, find first step with status != completed, resume from there.",
    "checkpoint_instruction": "Checkpoints require_user=true — present real output, ask user which improvements to implement, then proceed.",
    "compact_reminder": "Run /compact before prompts 8, 12, and 15 if session is long (per Implementation_Prompts.md).",
    "bird_data_note": "BIRD dataset must be downloaded before Checkpoint B. See Step 2.1 of Phase1_implementation_details.md.",
    "sampling_rule": "ALWAYS use stratified sampling from BIRD dev set (11 databases). For tests originally ≤10 questions: 33 total (3 per DB: 1 simple, 1 moderate, 1 challenging). For tests originally at 50 questions: 66 total (6 per DB: 2 simple, 2 moderate, 2 challenging). Always use random.seed(42) for reproducibility. NEVER use 'first N questions' — this hits only one database."
  }
}
