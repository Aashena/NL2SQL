{
  "phase": 1,
  "current_step": "prompt_11",
  "status": "in_progress",
  "steps": {
    "prompt_1": {
      "status": "completed",
      "tests_passed": true,
      "notes": "12/12 tests pass. Key decisions: (1) name column nullable to allow NULL rows per fixture. (2) Settings adds cache_llm_responses + log_level. (3) Timeout via threading.Thread + conn.interrupt() (not SIGALRM — macOS compat). (4) bird_loader tolerates alternative SQL field names. (5) 6 tests for bird_loader, 6 for database."
    },
    "prompt_2": {
      "status": "completed",
      "tests_passed": true,
      "notes": "31 tests pass. Key decisions: (1) datasketch.MinHash.hashvalues returns numpy.uint64 — convert via [int(v) for v in mh.hashvalues]. (2) SQLite type affinity: 5-rule algorithm. (3) avg_length only for TEXT; min/max/avg_value only for INTEGER/REAL/NUMERIC. (4) Empty tables: total_count=0, null_rate=0.0 guard. (5) JSON cache via dataclasses.asdict() + json.dump(default=str). (6) minhash_bands: character 3-grams; strings <=2 chars also update mh directly."
    },
    "prompt_3":      {"status": "completed",  "tests_passed": true,  "notes": "10/10 tests pass. Key decisions: (1) Batching via dict keyed by table_name, _BATCH_SIZE=6; (2) Retry decorator wraps inner _call() closure so tenacity reraises correctly; (3) cache_control ephemeral on system prompt block works with anthropic-0.83.0 SDK (passed as list of dicts); (4) Missing columns filled with deterministic default: 'The {col} field in the {table} table.'; (5) Length enforcement via simple slice [:200] / [:1000]; (6) JSON persistence via dataclasses.asdict() matching profiler pattern; (7) anthropic package was not yet installed — added to project dependencies."},
    "prompt_4":      {"status": "completed", "tests_passed": true, "notes": "11/11 tests pass (10 required + 1 bonus). Key decisions: (1) needs_quoting() uses re.compile('^[A-Za-z0-9_]+$') regex — returns True for any name containing spaces, parens, hyphens, slashes, etc. (2) DDL long_summary truncated to 120 chars with '...'; sample row values truncated to 50 chars. (3) NULL sample values: per-column 'NULL' string if sample_values is empty list. (4) Last column in DDL block: trailing comma stripped cleanly — handles both ', --' and bare ',' endings. (5) Markdown sample values: first 3, each truncated to 30 chars with '...'. (6) format_and_save_schemas() creates output_dir if needed, writes {db_id}_ddl.sql and {db_id}_markdown.md. (7) Offline script: --split / --step / --db / --force flags; tqdm progress bar; per-DB error catching; summary at end; indices step deferred to Prompt 6."},
    "checkpoint_A":  {"status": "completed", "requires_user": true, "notes": "All 64 unit tests pass. Improvements applied: (1) summarizer retry logic — on LLMError for batch>1, retries each column individually before falling back to default; (2) warning log emitted per table listing columns that got default fallback; (3) Gemini _parse_response guards for None candidates/content; (4) asyncio.run() fix in offline script. Gemini context caching implemented and tested. Ready for Prompt 5."},
    "prompt_5":      {"status": "completed", "tests_passed": true,  "notes": "10/10 tests pass. Key decisions: (1) LSH threshold lowered to 0.3 (from spec's 0.5) — true 3-gram Jaccard for 'Untied States'/'United States' is ~0.47, below 0.5 so the candidate would be missed; threshold=0.3 retrieves it and post-retrieval Jaccard re-ranking keeps precision high. (2) _minhashes dict maintained alongside MinHashLSH (LSH does not store MinHash objects) for Jaccard re-ranking. (3) _key_meta dict stores (table, column, value) per key for result assembly. (4) Numeric values stored as str(raw_val).strip(). (5) Duplicate keys caught via try/except ValueError on lsh.insert(). (6) sort key = (exact_match, similarity_score) descending — exact matches always float to top. (7) Serialisation via pickle of whole object. (8) Fuzzy test similarity assertion is > 0.4 (not > 0.5) reflecting true 3-gram Jaccard for single-transposition typos."},
    "prompt_6":      {"status": "completed",  "tests_passed": true, "notes": "24/24 tests pass. Key decisions: (1) FAISSIndex uses IndexFlatIP with L2-normalized embeddings (cosine similarity) for DBs <=1000 fields; IndexIVFFlat (nlist=min(32,n//10)) for larger DBs. (2) Similarity scores clipped to [0,1] with np.clip. (3) ExampleStore skeleton masking: collect ALL spans from original text (regex for [NUM], [STR], and spaCy NER for [ENTITY]), resolve overlaps greedily, apply in reverse order — this prevents double-masking (e.g. [NUM] being replaced by [ENTITY]). (4) ExampleStore.query fetches top_k*5 from FAISS then filters by db_id exclusion. (5) OfflineArtifacts pipeline: summary is explicitly saved to disk after summarize_database() call in case mock doesn't write it. (6) Example store is shared across all DBs (not per-DB), saved to indices/example_store.faiss + indices/example_store_metadata.json. (7) train_data empty → _build_example_store logs warning and returns empty store (no error). (8) Skeleton masking examples: 'Find students in Alameda County with GPA above 3.5' → 'Find students in [ENTITY] with [ENTITY] above [NUM]'."},
    "checkpoint_B":  {"status": "completed",  "requires_user": true, "notes": "11/11 BIRD dev databases processed (0 failures). Runtime: ~101 min. Fix applied: LSH caps distinct values per column at 50,000 — this prevented card_games from running 48+ min (now 10 min). Total LSH footprint: 10.86 GB. Issues: (1) LSH indexes non-text cols (addresses, lat/lng, IDs) — bloating index; fix pending user approval; (2) Summarizer batch failures on long-text cols — handled gracefully; (3) SSL event-loop-closed noise — benign. FAISS quality excellent. ExampleStore retrieves structurally similar examples correctly. See checkpoint_B_review/README.md."},
    "prompt_7":      {"status": "completed",  "tests_passed": true,  "notes": "17/17 tests pass. Key decisions: (1) CacheManager._make_key uses SHA256(model + json.dumps(messages, sort_keys=True)) — deterministic regardless of dict key ordering. (2) Cache files named {key[:16]}.json; full key stored inside entry for collision detection. (3) TTL checked at read time only — entries never evicted from disk proactively. (4) enabled=False short-circuits both get() and set() with no disk I/O. (5) cached() decorator extracts messages from first positional arg or 'messages' kwarg. (6) context_grounder uses TYPE_CHECKING guard for CellMatch/ExampleEntry/LSHIndex/ExampleStore imports to avoid circular import risks. (7) ground_context normalises empty/None evidence to the string 'None' before passing to LLM. (8) Deduplication by (table, column) keeps highest similarity_score match when two literals map to same column. (9) ExampleStore.query receives raw question — it performs skeleton masking internally. (10) temperature=0.0 used for deterministic extraction (spec doesn't specify, but 0 is appropriate for extraction tasks)."},
    "prompt_8":      {"status": "completed", "tests_passed": true, "notes": "12/12 tests pass. Key decisions: (1) Two-iteration LLM approach: S1=precise (only definitely needed), S2=recall (wider net). (2) FAISS pre-filter runs first (top_k=settings.faiss_top_k=30), then schema_hints from grounding context augment the candidate list. (3) PKs auto-added by parsing '-- Table:' + 'PRIMARY KEY' in DDL lines; FKs auto-added by parsing '-- Foreign keys:' comment lines. (4) Hallucination filter: each LLM-returned (table, column) is validated against available_field_set before inclusion. (5) S1 ⊆ S2 invariant guaranteed by: s2_extended = s2_extended | s1_extended at end of function. (6) DDL rendering: parse CREATE TABLE blocks per table using _parse_ddl_tables(), rebuild with only selected columns including corrected trailing comma. (7) Markdown rendering: parse ## Table: sections, filter rows by selected column names. (8) Prompt caching: system_block_2 (candidate field list) uses CacheableText(cache=True); system_block_1 (instruction) uses cache=False. (9) Exactly 2 API calls guaranteed even when no remaining candidates (second call made with empty candidate message). (10) All fields in s1_fields and s2_fields are sorted tuples for stable output."},
    "checkpoint_C":  {"status": "completed",  "requires_user": true, "notes": "Tested Ops 5+6 on first 5 BIRD dev questions (all california_schools). 4/5 completed OK. Fixes applied: (1) Gemini 'candidate.content.parts is None' → 'or []' guard; (2) context_grounder: multi-word literals now also query individual words ≥3 chars — fixed critical miss of frpm.County Name for Q1 ('Alameda County' → also queries 'Alameda' which exact-matches County Name). Results: Q1: County Name in S1 ✅; Q2: Educational Option Type in S1 ✅; Q3: Charter School, District Name, Zip in S1 ✅; Q4: FRPM Count, MailStreet in S1 ✅; Q5: Charter Funding Type, Charter School (Y/N), OpenDate, Phone all in S1 ✅. No hallucinations detected. 136/136 tests pass."},
    "prompt_9":      {"status": "completed",  "tests_passed": true, "notes": "10/10 tests pass. Key decisions: (1) clean_sql() strips ```sql ... ``` and ``` ... ``` fences via re.search with DOTALL, then strips trailing semicolons, then collapses whitespace with re.sub('\\s+', ' '). (2) validate_sql_syntax() checks for both SELECT and FROM case-insensitively — returns False for empty string. (3) build_base_prompt() formats question+evidence+cell_matches consistently; cell values shown as table.column = 'value'. (4) ReasoningGenerator uses asyncio.gather() for all 4 concurrent calls; diversity achieved via prompt variation (A1/A3=minimal, A2/A4=step-by-step). (5) No tool-use for reasoning generator — SQL comes in response.text, parsed by clean_sql(). (6) ThinkingConfig(enabled=True, budget_tokens=N) passed to generate(); adaptive budget: 4000 for <=2 tables, 6000 for <=4 tables, 8000 for 5+. (7) response.thinking → reasoning_trace field in SQLCandidate. (8) If response.text is None or clean_sql returns empty → error_flag=True, sql=''. (9) generator_id format: 'reasoning_A1' through 'reasoning_A4'; schema_format always 'ddl'. (10) System prompt includes full DDL schema via CacheableText(cache=True). (11) Step-by-step suffix replaces the final 'Write a SQL query...' line rather than appending after it."},
    "prompt_10":     {"status": "completed",  "tests_passed": true, "notes": "24/24 tests pass (10+10+4). Key decisions: (1) StandardAndComplexGenerator runs all 4 calls (B1_s1, B1_s2, B2_s1, B2_s2) concurrently via asyncio.gather(). (2) B1 uses model_fast, B2 uses model_powerful; neither uses extended thinking (thinking=None). (3) Both B1 and B2 use Markdown schemas (schema_format='markdown'); reasoning generator uses DDL. (4) B2 system prompt explicitly mentions CTEs and window functions to steer the model toward advanced patterns. (5) ICLGenerator formats examples as '## Example N' blocks; applies prompt caching to the examples block (CacheableText(cache=True)). (6) Token guard: len(formatted_examples) // 4 > 6000 → trim to first 6 examples. Used range(400) in test (3510 char SQL) to reliably exceed 6000 token estimate with 8 examples. (7) ICL instruction block has cache=False (schema content per-question), examples block has cache=True (shared across C1/C2/C3). (8) C1='Write the SQL query'; C2='First, identify which tables and joins...'; C3='What is the general SQL pattern...'. (9) Empty few_shot_examples → empty string examples block, still generates 3 candidates. (10) All 3 generators running concurrently produce 4+4+3=11 unique candidates with both S1 and S2 schemas represented."},
    "checkpoint_D":  {"status": "completed",  "requires_user": true, "notes": "PROPERLY re-run with stratified 33-question sample (3 per each of 11 BIRD dev DBs: 1 simple, 1 moderate, 1 challenging, random.seed(42)), runtime ~22.8 min. Oracle upper bound: 21/33=63.6% (Simple 81.8%, Moderate 54.5%, Challenging 54.5%). Gen success: A=100%/99%exec, B1=100%/95%exec, B2=87%/73%exec, C=87%/73%exec. Duplicate rate: 40% (133/332). 7 issues found: P0-1) schema_linker no try/except causes complete fail on MALFORMED_FUNCTION_CALL (Q23); P0-2) B2/C generators 13% empty response.text from Gemini; P1-3) S2 over-expansion (36 fields in european_football_2); P1-4) 40% duplicate rate when S1==S2; P1-5) hallucinated table.table.column in schema_linker (filtered correctly); P2-6) evidence not prominent in generator prompts; P2-7) SentenceTransformer reloaded per question. Awaiting user decisions (A-D) before prompt_11. See checkpoint_D_review/inspection_report.md."},
    "prompt_11":     {"status": "pending",   "tests_passed": null, "notes": "Op 8: query_fixer.py + test_query_fixer.py (12 tests)"},
    "prompt_12":     {"status": "pending",   "tests_passed": null, "notes": "Op 9: adaptive_selector.py + test_adaptive_selector.py (12 tests)"},
    "checkpoint_E":  {"status": "pending",   "requires_user": true, "notes": "Manual e2e on 33 questions (3 per each of 11 databases: 1 simple, 1 moderate, 1 challenging) calling each component in sequence; review interfaces before pipeline wiring"},
    "prompt_13":     {"status": "pending",   "tests_passed": null, "notes": "Pipeline: online_pipeline.py + test_online_pipeline.py + run_evaluation.py"},
    "prompt_14":     {"status": "pending",   "tests_passed": null, "notes": "Evaluation: evaluator.py + metrics.py + analyze_results.py + test_evaluator.py"},
    "prompt_15":     {"status": "pending",   "tests_passed": null, "notes": "E2E tests: test_bird_mini.py + test_bird_full.py + run 66-question smoke test (6 per each of 11 databases: 2 simple, 2 moderate, 2 challenging); seeded random selection random.seed(42)"},
    "checkpoint_final": {"status": "pending", "requires_user": true, "notes": "Full BIRD dev evaluation (1534 questions); target EX >= 68%"}
  },
  "notes": {
    "resume_instruction": "Say 'continue implementation' at start of new session. Read this file, find first step with status != completed, resume from there.",
    "checkpoint_instruction": "Checkpoints require_user=true — present real output, ask user which improvements to implement, then proceed.",
    "compact_reminder": "Run /compact before prompts 8, 12, and 15 if session is long (per Implementation_Prompts.md).",
    "bird_data_note": "BIRD dataset must be downloaded before Checkpoint B. See Step 2.1 of Phase1_implementation_details.md.",
    "sampling_rule": "ALWAYS use stratified sampling from BIRD dev set (11 databases). For tests originally ≤10 questions: 33 total (3 per DB: 1 simple, 1 moderate, 1 challenging). For tests originally at 50 questions: 66 total (6 per DB: 2 simple, 2 moderate, 2 challenging). Always use random.seed(42) for reproducibility. NEVER use 'first N questions' — this hits only one database."
  }
}
